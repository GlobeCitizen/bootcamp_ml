{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 2.711028065632692\n",
      "epoch 150 : loss 0.935573616989867\n",
      "epoch 300 : loss 0.9291594948612015\n",
      "epoch 450 : loss 0.9268586737297158\n",
      "epoch 600 : loss 0.9257248476307497\n",
      "epoch 750 : loss 0.9250155938147973\n",
      "epoch 900 : loss 0.9245111890003629\n",
      "epoch 1050 : loss 0.9241277108985992\n",
      "epoch 1200 : loss 0.9238243123279398\n",
      "epoch 1350 : loss 0.9235774051604406\n",
      "epoch 1500 : loss 0.9233719838154981\n",
      "epoch 1650 : loss 0.9231979667181069\n",
      "epoch 1800 : loss 0.9230483538800716\n",
      "epoch 1950 : loss 0.9229181236337788\n",
      "epoch 2100 : loss 0.9228036104725624\n",
      "epoch 2250 : loss 0.9227020820487567\n",
      "epoch 2400 : loss 0.9226114387531494\n",
      "epoch 2550 : loss 0.9225300290078275\n",
      "epoch 2700 : loss 0.9224565596310702\n",
      "epoch 2850 : loss 0.9223899740098948\n",
      "epoch 3000 : loss 0.9223293803180658\n",
      "epoch 3150 : loss 0.9222740904727086\n",
      "epoch 3300 : loss 0.9222234718722943\n",
      "epoch 3450 : loss 0.9221770062453721\n",
      "epoch 3600 : loss 0.9221342604079639\n",
      "epoch 3750 : loss 0.9220948381943443\n",
      "epoch 3900 : loss 0.9220584049297934\n",
      "epoch 4050 : loss 0.9220246790684583\n",
      "epoch 4200 : loss 0.9219933910964939\n",
      "epoch 4350 : loss 0.9219643243752265\n",
      "epoch 4500 : loss 0.9219372646699326\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "\n",
    "class LogisticRegressionBatchGd:\n",
    "    def __init__(self, alpha=0.001, max_iter=1000, verbose=False, learning_rate='constant'):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate\n",
    "        self.thetas = []\n",
    "        self.loss = []\n",
    "\n",
    "    def __sigmoid_(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __vec_log_loss_(self, y_true, y_pred, m, eps=1e-15):\n",
    "        if m > 1:\n",
    "            return -1 / m * np.dot(np.ones((1, m)), ((y_true * np.log(y_pred+eps)) + (1 - y_true) * np.log(1- y_pred+eps)))[0]\n",
    "        else:\n",
    "            return -1 / m * np.dot(np.ones((1, m)), ((y_true * np.log(y_pred+eps)) + (1 - y_true) * np.log(1- y_pred+ eps)))[0][0]\n",
    "\n",
    "    def __vec_log_gradient_(self, x, y_true, y_pred):\n",
    "        return np.dot((y_pred - y_true), x)\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "\n",
    "        \n",
    "        for iters in range(self.max_iter + 1):\n",
    "            y_pred = self.predict(x_train)\n",
    "            self.thetas -= self.alpha * self.__vec_log_gradient_(x_train, y_train, y_pred)\n",
    "            loss = self.__vec_log_loss_(y_train, y_pred, len(y_train), eps=1e-15)\n",
    "            if self.verbose == True and iters % 150 == 0:\n",
    "                print('epoch {} : loss {}'.format(iters, loss))\n",
    "\n",
    "    def predict(self, x_train):\n",
    "        z = np.dot(x_train, self.thetas)\n",
    "        return self.__sigmoid_(z)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        score = 0\n",
    "        for x,y in zip(y_pred,y):\n",
    "            if x == y:\n",
    "                score += 1\n",
    "        return score / y_test.size\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('../resources//train_dataset_clean.csv', delimiter=',',\n",
    "header=None, index_col=False)\n",
    "x_train, y_train = np.array(df_train.iloc[:, 1:82]), df_train.iloc[:, 0]\n",
    "df_test = pd.read_csv('../resources/test_dataset_clean.csv', delimiter=',', header=None,\n",
    "index_col=False)\n",
    "x_test, y_test = np.array(df_test.iloc[:, 1:82]), df_test.iloc[:, 0]\n",
    "x_train = np.insert(x_train, 0, 1.0, axis=1)\n",
    "\n",
    "x_test = np.insert(x_test, 0, 1.0, axis=1)\n",
    "# We set our model with our hyperparameters : alpha, max_iter, verbose and learning_rate\n",
    "model = LogisticRegressionBatchGd(alpha=0.001, max_iter=15000, verbose=True, learning_rate='constant')\n",
    "# We fit our model to our dataset and display the score for the train and test datasets\n",
    "model.thetas = np.ones(x_train.shape[1])\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Score on train dataset : {model.score(x_train, y_train)}')\n",
    "y_pred = model.predict(x_train)\n",
    "print(y_pred.size)\n",
    "print(y_test.size)\n",
    "print(f'Score on test dataset : {(y_pred == y_test).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(model._LogisticRegressionBatchGd__sigmoid_(np.dot(x_train, model.thetas)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99767515, 0.91583944, 0.95848459, ..., 0.99633604, 0.5857667 ,\n",
       "       0.99925398])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._LogisticRegressionBatchGd__sigmoid_(np.dot(x_train, model.thetas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7841"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.count_nonzero(y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83710898,  1.13473876, -0.14592048, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.04264203, -0.42005962, -0.14592048, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.05704673, -1.19745882, -0.14592048, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.42360965, -0.42005962, -0.14592048, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.21564337, -0.42005962, -0.14592048, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.98373415, -0.42005962,  1.88842434, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
